{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e3e4be-9641-401a-9c93-66dfd6855f33",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8422ffd2-3570-4d01-b433-1bd22c3181ed",
   "metadata": {},
   "source": [
    "# A Transformer Model for Language Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6e4cb7-2525-4f4d-b4d3-22d82d9d2fcc",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li><a href=\"#Background\">Background</a></li>\n",
    "    <li><a href=\"#Setup\">Setup</a></li>\n",
    "    <li><a href=\"#DataLoader\">DataLoader</a></li>\n",
    "    <li><a href=\"#Transformer-architecture-for-language-translation\">Transformer architecture for language translation</a></li>\n",
    "    <li><a href=\"#Training-the-model\">Training the model</a></li>\n",
    "    <li><a href=\"#Translation-and-evaluation\">Translation and evaluation</a></li>\n",
    "    <li><a href=\"#Exercise:-Translating-a-document\">Exercise: Translating a document</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681fca4-b3bf-4cfc-adf3-227e0ff2d68a",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Describe the transformer architecture\n",
    "- Build a translation model from scratch using PyTorch:\n",
    "    - Preprocess textual data\n",
    "    - Design the transformer architecture\n",
    "    - Train the model using parallel computing\n",
    "    - Evaluate the model performance\n",
    "    - Generate translation\n",
    "- Translate a PDF document from German to English\n",
    "\n",
    "# Background\n",
    "In today’s interconnected world, the ability to break language barriers is more valuable than ever. Whether it's for expanding business reach, accessing information in different languages, or connecting with people across the globe, language translation plays a crucial role. This is where transformer model for language translation lab steps in, letting you delve into the world of neural machine translation – a field at the forefront of overcoming language barriers. You'll explore how to implement a transformer model to translate text from one language to another.\n",
    "\n",
    "## Why transformers?\n",
    "In the field of natural language processing (NLP), you often deal with sequential data, like sentences. Before transformers, the most common models used were Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs) networks. These models process data sequentially, meaning they read a sentence word by word, one after the other. This makes them slow and less efficient for long sequences. Also, RNNs and LSTMs can struggle to keep track of information from earlier in the sequence, which is vital in understanding context.\n",
    "\n",
    "Transformers introduced a new way of processing sequences. Instead of reading word by word in order, they can look at the entire sequence at once. This approach makes them faster and more efficient. They can also better understand the context of each word in a sentence, no matter how long it is.\n",
    "\n",
    "![translation](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/Translation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700ad5a-9473-4263-ab77-a20a6eb78a32",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Installing required libraries\n",
    "Before you start, let's make sure you have all the necessary libraries installed. You can run the following commands to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "196980bf-7d1b-4557-a6ee-d53686d74b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata==0.5.1\n",
      "  Downloading torchdata-0.5.1-cp310-cp310-win_amd64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\user\\myenv\\lib\\site-packages (from torchdata==0.5.1) (2.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\myenv\\lib\\site-packages (from torchdata==0.5.1) (2.32.4)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in c:\\users\\user\\myenv\\lib\\site-packages (from torchdata==0.5.1) (2.8.2)\n",
      "Collecting torch==1.13.1 (from torchdata==0.5.1)\n",
      "  Using cached torch-1.13.1-cp310-cp310-win_amd64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\myenv\\lib\\site-packages (from torch==1.13.1->torchdata==0.5.1) (4.14.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\myenv\\lib\\site-packages (from portalocker>=2.0.0->torchdata==0.5.1) (310)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\myenv\\lib\\site-packages (from requests->torchdata==0.5.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\myenv\\lib\\site-packages (from requests->torchdata==0.5.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\myenv\\lib\\site-packages (from requests->torchdata==0.5.1) (2025.7.9)\n",
      "Downloading torchdata-0.5.1-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.5/1.2 MB 840.2 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 0.8/1.2 MB 819.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.0/1.2 MB 839.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 806.7 kB/s eta 0:00:00\n",
      "Using cached torch-1.13.1-cp310-cp310-win_amd64.whl (162.6 MB)\n",
      "Installing collected packages: torch, torchdata\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.2.2\n",
      "\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Uninstalling torch-2.2.2:\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "  Attempting uninstall: torchdata\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Found existing installation: torchdata 0.7.1\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "    Uninstalling torchdata-0.7.1:\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "      Successfully uninstalled torchdata-0.7.1\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   -------------------- ------------------- 1/2 [torchdata]\n",
      "   ---------------------------------------- 2/2 [torchdata]\n",
      "\n",
      "Successfully installed torch-1.13.1 torchdata-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0+cpu requires torch==2.2.0, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.17.2 requires torch==2.2.2, but you have torch 1.13.1 which is incompatible.\n",
      "torchvision 0.17.0+cpu requires torch==2.2.0, but you have torch 1.13.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==3.7.2\n",
      "  Downloading spacy-3.7.2-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy==3.7.2)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy==3.7.2)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy==3.7.2)\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy==3.7.2)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy==3.7.2)\n",
      "  Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy==3.7.2)\n",
      "  Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy==3.7.2)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy==3.7.2)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy==3.7.2)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.2)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy==3.7.2)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.7.2)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy==3.7.2)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.7.2)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy==3.7.2) (1.26.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.7.9)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy==3.7.2)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.2) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy==3.7.2)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.2)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\myenv\\lib\\site-packages (from jinja2->spacy==3.7.2) (3.0.2)\n",
      "Downloading spacy-3.7.2-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.1 MB 796.8 kB/s eta 0:00:15\n",
      "   -- ------------------------------------- 0.8/12.1 MB 817.9 kB/s eta 0:00:14\n",
      "   -- ------------------------------------- 0.8/12.1 MB 817.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.0/12.1 MB 838.4 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 838.9 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 838.9 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 830.1 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 1.8/12.1 MB 838.9 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.1/12.1 MB 844.8 kB/s eta 0:00:12\n",
      "   ------ --------------------------------- 2.1/12.1 MB 844.8 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.4/12.1 MB 838.9 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.1 MB 838.7 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.6/12.1 MB 838.7 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.9/12.1 MB 838.9 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 838.7 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 838.7 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 835.3 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 3.7/12.1 MB 835.5 kB/s eta 0:00:11\n",
      "   ------------ --------------------------- 3.7/12.1 MB 835.5 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 3.9/12.1 MB 835.8 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.2/12.1 MB 835.9 kB/s eta 0:00:10\n",
      "   ------------- -------------------------- 4.2/12.1 MB 835.9 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 4.5/12.1 MB 836.2 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 4.7/12.1 MB 836.3 kB/s eta 0:00:09\n",
      "   --------------- ------------------------ 4.7/12.1 MB 836.3 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 838.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 836.5 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 836.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 5.5/12.1 MB 838.9 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 5.8/12.1 MB 838.8 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 5.8/12.1 MB 838.8 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 6.0/12.1 MB 836.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 6.3/12.1 MB 838.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 6.3/12.1 MB 838.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 6.6/12.1 MB 837.1 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 837.1 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 837.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 837.2 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 7.3/12.1 MB 837.2 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 7.3/12.1 MB 837.2 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 7.6/12.1 MB 835.8 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 7.9/12.1 MB 837.3 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 7.9/12.1 MB 837.3 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 7.9/12.1 MB 837.3 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.1/12.1 MB 821.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 8.4/12.1 MB 820.3 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 8.4/12.1 MB 820.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 820.9 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 821.4 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 821.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 9.2/12.1 MB 820.7 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.4/12.1 MB 820.1 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 9.4/12.1 MB 820.1 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.7/12.1 MB 821.7 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/12.1 MB 822.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/12.1 MB 822.2 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.2/12.1 MB 822.6 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 823.0 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 823.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 823.4 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.1 MB 823.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/12.1 MB 823.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.3/12.1 MB 822.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 822.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 812.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 748.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 681.8 kB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 840.2 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 860.9 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 860.9 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 825.2 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 825.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 828.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 830.6 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 830.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/2.0 MB 825.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 802.7 kB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/632.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.3 kB ? eta -:--:--\n",
      "   -------------------------------------- 632.3/632.3 kB 847.0 kB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 882.6 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 860.9 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 868.0 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 868.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 860.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 822.1 kB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.6 MB 840.2 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 860.9 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.0/6.6 MB 853.0 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.0/6.6 MB 853.0 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.3/6.6 MB 838.9 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.6/6.6 MB 839.1 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.6/6.6 MB 839.1 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 846.1 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.1/6.6 MB 851.1 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 2.1/6.6 MB 851.1 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 2.4/6.6 MB 844.3 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 2.6/6.6 MB 843.5 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.6/6.6 MB 843.5 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 838.9 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.1/6.6 MB 842.7 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 3.1/6.6 MB 842.7 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.4/6.6 MB 842.5 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 839.0 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 839.0 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 841.9 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 4.2/6.6 MB 844.5 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.2/6.6 MB 844.5 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.5/6.6 MB 844.3 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 843.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 843.9 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 5.0/6.6 MB 843.7 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.2/6.6 MB 843.3 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.2/6.6 MB 843.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.5/6.6 MB 838.9 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 840.8 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 840.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 6.0/6.6 MB 840.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 840.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 840.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 838.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 827.1 kB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 882.6 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.5/5.4 MB 882.6 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 838.9 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.0/5.4 MB 867.1 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.0/5.4 MB 867.1 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.3/5.4 MB 860.2 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 855.6 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 855.6 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 1.8/5.4 MB 824.8 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 1.8/5.4 MB 824.8 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.1/5.4 MB 827.0 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 828.3 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 828.3 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 2.6/5.4 MB 829.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 2.9/5.4 MB 818.2 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 2.9/5.4 MB 818.2 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 823.9 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.4/5.4 MB 825.0 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.4/5.4 MB 825.0 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.4 MB 829.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 818.3 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 818.3 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.4 MB 819.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.4 MB 820.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 819.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 822.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 822.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 823.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 817.7 kB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/24 [cymem]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   - --------------------------------------  1/24 [wasabi]\n",
      "   --- ------------------------------------  2/24 [typing-inspection]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/24 [spacy-loggers]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "  Attempting uninstall: smart-open\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "    Found existing installation: smart_open 7.3.0.post1\n",
      "   ------ ---------------------------------  4/24 [spacy-legacy]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "    Uninstalling smart_open-7.3.0.post1:\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "      Successfully uninstalled smart_open-7.3.0.post1\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   -------- -------------------------------  5/24 [smart-open]\n",
      "   ---------- -----------------------------  6/24 [pydantic-core]\n",
      "   ---------- -----------------------------  6/24 [pydantic-core]\n",
      "   ---------- -----------------------------  6/24 [pydantic-core]\n",
      "   ----------- ----------------------------  7/24 [murmurhash]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   --------------- ------------------------  9/24 [cloudpathlib]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ---------------- ----------------------- 10/24 [click]\n",
      "   ------------------ --------------------- 11/24 [catalogue]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   -------------------- ------------------- 12/24 [blis]\n",
      "   --------------------- ------------------ 13/24 [annotated-types]\n",
      "   ----------------------- ---------------- 14/24 [typer]\n",
      "   ----------------------- ---------------- 14/24 [typer]\n",
      "   ----------------------- ---------------- 14/24 [typer]\n",
      "   ----------------------- ---------------- 14/24 [typer]\n",
      "   ----------------------- ---------------- 14/24 [typer]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   ------------------------- -------------- 15/24 [srsly]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   -------------------------- ------------- 16/24 [pydantic]\n",
      "   ---------------------------- ----------- 17/24 [preshed]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------ --------- 18/24 [language-data]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   ------------------------------- -------- 19/24 [langcodes]\n",
      "   --------------------------------- ------ 20/24 [confection]\n",
      "   --------------------------------- ------ 20/24 [confection]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ----------------------------------- ---- 21/24 [weasel]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   ------------------------------------ --- 22/24 [thinc]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   -------------------------------------- - 23/24 [spacy]\n",
      "   ---------------------------------------- 24/24 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.16.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.2.5 typer-0.9.4 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/14.6 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/14.6 MB 212.4 kB/s eta 0:01:07\n",
      "     - ------------------------------------- 0.5/14.6 MB 212.4 kB/s eta 0:01:07\n",
      "     -- ------------------------------------ 0.8/14.6 MB 297.0 kB/s eta 0:00:47\n",
      "     -- ------------------------------------ 0.8/14.6 MB 297.0 kB/s eta 0:00:47\n",
      "     -- ------------------------------------ 1.0/14.6 MB 375.7 kB/s eta 0:00:37\n",
      "     --- ----------------------------------- 1.3/14.6 MB 432.9 kB/s eta 0:00:31\n",
      "     --- ----------------------------------- 1.3/14.6 MB 432.9 kB/s eta 0:00:31\n",
      "     ---- ---------------------------------- 1.6/14.6 MB 482.2 kB/s eta 0:00:28\n",
      "     ---- ---------------------------------- 1.8/14.6 MB 518.9 kB/s eta 0:00:25\n",
      "     ---- ---------------------------------- 1.8/14.6 MB 518.9 kB/s eta 0:00:25\n",
      "     ----- --------------------------------- 2.1/14.6 MB 548.9 kB/s eta 0:00:23\n",
      "     ------ -------------------------------- 2.4/14.6 MB 573.6 kB/s eta 0:00:22\n",
      "     ------ -------------------------------- 2.4/14.6 MB 573.6 kB/s eta 0:00:22\n",
      "     ------ -------------------------------- 2.6/14.6 MB 596.8 kB/s eta 0:00:21\n",
      "     ------- ------------------------------- 2.9/14.6 MB 614.6 kB/s eta 0:00:20\n",
      "     ------- ------------------------------- 2.9/14.6 MB 614.6 kB/s eta 0:00:20\n",
      "     -------- ------------------------------ 3.1/14.6 MB 632.1 kB/s eta 0:00:19\n",
      "     --------- ----------------------------- 3.4/14.6 MB 643.3 kB/s eta 0:00:18\n",
      "     --------- ----------------------------- 3.4/14.6 MB 643.3 kB/s eta 0:00:18\n",
      "     --------- ----------------------------- 3.7/14.6 MB 657.0 kB/s eta 0:00:17\n",
      "     ---------- ---------------------------- 3.9/14.6 MB 667.3 kB/s eta 0:00:17\n",
      "     ---------- ---------------------------- 3.9/14.6 MB 667.3 kB/s eta 0:00:17\n",
      "     ----------- --------------------------- 4.2/14.6 MB 676.6 kB/s eta 0:00:16\n",
      "     ----------- --------------------------- 4.5/14.6 MB 684.8 kB/s eta 0:00:15\n",
      "     ----------- --------------------------- 4.5/14.6 MB 684.8 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.7/14.6 MB 692.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 5.0/14.6 MB 697.5 kB/s eta 0:00:14\n",
      "     ------------- ------------------------- 5.0/14.6 MB 697.5 kB/s eta 0:00:14\n",
      "     ------------- ------------------------- 5.2/14.6 MB 706.8 kB/s eta 0:00:14\n",
      "     -------------- ------------------------ 5.5/14.6 MB 712.4 kB/s eta 0:00:13\n",
      "     -------------- ------------------------ 5.5/14.6 MB 712.4 kB/s eta 0:00:13\n",
      "     --------------- ----------------------- 5.8/14.6 MB 717.5 kB/s eta 0:00:13\n",
      "     ---------------- ---------------------- 6.0/14.6 MB 722.3 kB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 6.3/14.6 MB 728.1 kB/s eta 0:00:12\n",
      "     ---------------- ---------------------- 6.3/14.6 MB 728.1 kB/s eta 0:00:12\n",
      "     ----------------- --------------------- 6.6/14.6 MB 730.8 kB/s eta 0:00:12\n",
      "     ------------------ -------------------- 6.8/14.6 MB 735.9 kB/s eta 0:00:11\n",
      "     ------------------ -------------------- 6.8/14.6 MB 735.9 kB/s eta 0:00:11\n",
      "     ------------------ -------------------- 7.1/14.6 MB 739.4 kB/s eta 0:00:11\n",
      "     ------------------- ------------------- 7.3/14.6 MB 742.6 kB/s eta 0:00:10\n",
      "     ------------------- ------------------- 7.3/14.6 MB 742.6 kB/s eta 0:00:10\n",
      "     -------------------- ------------------ 7.6/14.6 MB 745.7 kB/s eta 0:00:10\n",
      "     -------------------- ------------------ 7.9/14.6 MB 748.5 kB/s eta 0:00:10\n",
      "     -------------------- ------------------ 7.9/14.6 MB 748.5 kB/s eta 0:00:10\n",
      "     --------------------- ----------------- 8.1/14.6 MB 751.3 kB/s eta 0:00:09\n",
      "     ---------------------- ---------------- 8.4/14.6 MB 754.9 kB/s eta 0:00:09\n",
      "     ---------------------- ---------------- 8.4/14.6 MB 754.9 kB/s eta 0:00:09\n",
      "     ----------------------- --------------- 8.7/14.6 MB 757.2 kB/s eta 0:00:08\n",
      "     ----------------------- --------------- 8.9/14.6 MB 760.5 kB/s eta 0:00:08\n",
      "     ----------------------- --------------- 8.9/14.6 MB 760.5 kB/s eta 0:00:08\n",
      "     ------------------------ -------------- 9.2/14.6 MB 762.6 kB/s eta 0:00:08\n",
      "     ------------------------- ------------- 9.4/14.6 MB 764.6 kB/s eta 0:00:07\n",
      "     ------------------------- ------------- 9.4/14.6 MB 764.6 kB/s eta 0:00:07\n",
      "     ------------------------- ------------- 9.7/14.6 MB 766.5 kB/s eta 0:00:07\n",
      "     ------------------------- ------------ 10.0/14.6 MB 768.3 kB/s eta 0:00:07\n",
      "     ------------------------- ------------ 10.0/14.6 MB 768.3 kB/s eta 0:00:07\n",
      "     -------------------------- ----------- 10.2/14.6 MB 770.0 kB/s eta 0:00:06\n",
      "     --------------------------- ---------- 10.5/14.6 MB 773.5 kB/s eta 0:00:06\n",
      "     --------------------------- ---------- 10.5/14.6 MB 773.5 kB/s eta 0:00:06\n",
      "     --------------------------- ---------- 10.7/14.6 MB 774.9 kB/s eta 0:00:06\n",
      "     ---------------------------- --------- 11.0/14.6 MB 776.4 kB/s eta 0:00:05\n",
      "     ----------------------------- -------- 11.3/14.6 MB 778.6 kB/s eta 0:00:05\n",
      "     ----------------------------- -------- 11.3/14.6 MB 778.6 kB/s eta 0:00:05\n",
      "     ----------------------------- -------- 11.5/14.6 MB 779.9 kB/s eta 0:00:04\n",
      "     ------------------------------ ------- 11.8/14.6 MB 781.2 kB/s eta 0:00:04\n",
      "     ------------------------------ ------- 11.8/14.6 MB 781.2 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 12.1/14.6 MB 782.4 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 12.3/14.6 MB 783.5 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 12.3/14.6 MB 783.5 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 12.6/14.6 MB 784.6 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 12.8/14.6 MB 785.7 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 12.8/14.6 MB 785.7 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 13.1/14.6 MB 786.7 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 13.4/14.6 MB 787.7 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 13.4/14.6 MB 787.7 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 13.6/14.6 MB 788.6 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 13.9/14.6 MB 789.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 13.9/14.6 MB 789.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 14.2/14.6 MB 790.4 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------  14.4/14.6 MB 791.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 14.6/14.6 MB 686.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\user\\myenv\\lib\\site-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.7.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\myenv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\myenv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\user\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\myenv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 882.6 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.8/12.8 MB 884.1 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 868.0 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 868.0 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.3/12.8 MB 860.9 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.3/12.8 MB 860.9 kB/s eta 0:00:14\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 839.1 kB/s eta 0:00:14\n",
      "     ----- --------------------------------- 1.8/12.8 MB 853.4 kB/s eta 0:00:13\n",
      "     ------ -------------------------------- 2.1/12.8 MB 851.1 kB/s eta 0:00:13\n",
      "     ------ -------------------------------- 2.1/12.8 MB 851.1 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.4/12.8 MB 849.7 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 853.3 kB/s eta 0:00:12\n",
      "     ------- ------------------------------- 2.6/12.8 MB 853.3 kB/s eta 0:00:12\n",
      "     -------- ------------------------------ 2.9/12.8 MB 851.7 kB/s eta 0:00:12\n",
      "     --------- ----------------------------- 3.1/12.8 MB 850.6 kB/s eta 0:00:12\n",
      "     --------- ----------------------------- 3.1/12.8 MB 850.6 kB/s eta 0:00:12\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 853.2 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.7/12.8 MB 855.4 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 855.4 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.9/12.8 MB 851.1 kB/s eta 0:00:11\n",
      "     ------------ -------------------------- 4.2/12.8 MB 850.2 kB/s eta 0:00:11\n",
      "     ------------ -------------------------- 4.2/12.8 MB 850.2 kB/s eta 0:00:11\n",
      "     ------------- ------------------------- 4.5/12.8 MB 849.6 kB/s eta 0:00:10\n",
      "     -------------- ------------------------ 4.7/12.8 MB 851.4 kB/s eta 0:00:10\n",
      "     -------------- ------------------------ 4.7/12.8 MB 851.4 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.0/12.8 MB 850.7 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.2/12.8 MB 847.8 kB/s eta 0:00:09\n",
      "     --------------- ----------------------- 5.2/12.8 MB 847.8 kB/s eta 0:00:09\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 849.5 kB/s eta 0:00:09\n",
      "     ----------------- --------------------- 5.8/12.8 MB 846.9 kB/s eta 0:00:09\n",
      "     ----------------- --------------------- 5.8/12.8 MB 846.9 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 848.5 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.3/12.8 MB 848.1 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.3/12.8 MB 848.1 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 847.7 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 847.4 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 847.4 kB/s eta 0:00:08\n",
      "     --------------------- ----------------- 7.1/12.8 MB 847.0 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 848.4 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 848.4 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 848.0 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.9/12.8 MB 847.7 kB/s eta 0:00:06\n",
      "     ----------------------- --------------- 7.9/12.8 MB 847.7 kB/s eta 0:00:06\n",
      "     ------------------------ -------------- 8.1/12.8 MB 847.4 kB/s eta 0:00:06\n",
      "     ------------------------- ------------- 8.4/12.8 MB 847.1 kB/s eta 0:00:06\n",
      "     ------------------------- ------------- 8.4/12.8 MB 847.1 kB/s eta 0:00:06\n",
      "     ------------------------- ------------- 8.4/12.8 MB 847.1 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     -------------------------- ------------ 8.7/12.8 MB 827.3 kB/s eta 0:00:06\n",
      "     --------------------------- ----------- 8.9/12.8 MB 686.9 kB/s eta 0:00:06\n",
      "     --------------------------- ----------- 8.9/12.8 MB 686.9 kB/s eta 0:00:06\n",
      "     --------------------------- ----------- 9.2/12.8 MB 687.3 kB/s eta 0:00:06\n",
      "     --------------------------- ----------- 9.2/12.8 MB 687.3 kB/s eta 0:00:06\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 690.8 kB/s eta 0:00:05\n",
      "     ----------------------------- --------- 9.7/12.8 MB 694.3 kB/s eta 0:00:05\n",
      "     ----------------------------- --------- 9.7/12.8 MB 694.3 kB/s eta 0:00:05\n",
      "     ----------------------------- -------- 10.0/12.8 MB 697.5 kB/s eta 0:00:05\n",
      "     ------------------------------ ------- 10.2/12.8 MB 700.6 kB/s eta 0:00:04\n",
      "     ------------------------------ ------- 10.2/12.8 MB 700.6 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.5/12.8 MB 703.6 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.7/12.8 MB 707.2 kB/s eta 0:00:03\n",
      "     ------------------------------- ------ 10.7/12.8 MB 707.2 kB/s eta 0:00:03\n",
      "     -------------------------------- ----- 11.0/12.8 MB 709.9 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 711.1 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 711.1 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 11.5/12.8 MB 710.8 kB/s eta 0:00:02\n",
      "     ---------------------------------- --- 11.5/12.8 MB 710.8 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 713.2 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 715.6 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 715.6 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 715.9 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 718.2 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 718.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 719.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\myenv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\myenv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.7.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\myenv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\myenv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\user\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\myenv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber==0.9.0\n",
      "  Downloading pdfplumber-0.9.0-py3-none-any.whl.metadata (35 kB)\n",
      "Collecting pdfminer.six==20221105 (from pdfplumber==0.9.0)\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\myenv\\lib\\site-packages (from pdfplumber==0.9.0) (11.3.0)\n",
      "Collecting Wand>=0.6.10 (from pdfplumber==0.9.0)\n",
      "  Downloading Wand-0.6.13-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\user\\myenv\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber==0.9.0) (3.4.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20221105->pdfplumber==0.9.0)\n",
      "  Downloading cryptography-45.0.5-cp37-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
      "Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 837.5 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 838.9 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 838.9 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.0/5.6 MB 825.2 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.3/5.6 MB 838.9 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.3/5.6 MB 838.9 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 838.6 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 1.8/5.6 MB 831.8 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.1/5.6 MB 838.7 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.1/5.6 MB 838.7 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 844.3 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.6/5.6 MB 843.5 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.6/5.6 MB 843.5 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 2.9/5.6 MB 843.2 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 846.6 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 846.6 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 3.4/5.6 MB 845.9 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 3.7/5.6 MB 845.4 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 3.7/5.6 MB 845.4 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 3.9/5.6 MB 841.9 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 844.5 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 844.5 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.5/5.6 MB 846.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.7/5.6 MB 846.3 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.7/5.6 MB 846.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 845.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 845.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 845.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 845.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 835.4 kB/s eta 0:00:00\n",
      "Downloading cryptography-45.0.5-cp37-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.4 MB 932.9 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.5/3.4 MB 932.9 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.8/3.4 MB 907.1 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.0/3.4 MB 882.6 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.0/3.4 MB 882.6 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 1.3/3.4 MB 871.6 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.6/3.4 MB 864.6 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.6/3.4 MB 864.6 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.8/3.4 MB 860.4 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.1/3.4 MB 870.1 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.1/3.4 MB 870.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.4/3.4 MB 854.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.6/3.4 MB 862.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.9/3.4 MB 860.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.9/3.4 MB 860.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.1/3.4 MB 862.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 855.5 kB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Downloading Wand-0.6.13-py2.py3-none-any.whl (143 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: Wand, pycparser, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "\n",
      "   ---------------------------------------- 0/6 [Wand]\n",
      "   ---------------------------------------- 0/6 [Wand]\n",
      "   ---------------------------------------- 0/6 [Wand]\n",
      "   ---------------------------------------- 0/6 [Wand]\n",
      "   ---------------------------------------- 0/6 [Wand]\n",
      "   ------ --------------------------------- 1/6 [pycparser]\n",
      "   ------ --------------------------------- 1/6 [pycparser]\n",
      "   ------ --------------------------------- 1/6 [pycparser]\n",
      "   ------ --------------------------------- 1/6 [pycparser]\n",
      "   ------------- -------------------------- 2/6 [cffi]\n",
      "   ------------- -------------------------- 2/6 [cffi]\n",
      "   ------------- -------------------------- 2/6 [cffi]\n",
      "   ------------- -------------------------- 2/6 [cffi]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------- ------------------- 3/6 [cryptography]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   -------------------------- ------------- 4/6 [pdfminer.six]\n",
      "   --------------------------------- ------ 5/6 [pdfplumber]\n",
      "   --------------------------------- ------ 5/6 [pdfplumber]\n",
      "   --------------------------------- ------ 5/6 [pdfplumber]\n",
      "   --------------------------------- ------ 5/6 [pdfplumber]\n",
      "   ---------------------------------------- 6/6 [pdfplumber]\n",
      "\n",
      "Successfully installed Wand-0.6.13 cffi-1.17.1 cryptography-45.0.5 pdfminer.six-20221105 pdfplumber-0.9.0 pycparser-2.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fpdf==1.7.2\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: fpdf\n",
      "  Building wheel for fpdf (pyproject.toml): started\n",
      "  Building wheel for fpdf (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40769 sha256=1454abf1961c45dab34833db30dc5d73a508d94e0726ba7c1144436c328c713b\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\f9\\95\\ba\\f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
      "Successfully built fpdf\n",
      "Installing collected packages: fpdf\n",
      "Successfully installed fpdf-1.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\user\\myenv\\lib\\site-packages)\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchdata==0.5.1\n",
    "!pip install -U spacy==3.7.2\n",
    "!pip install -Uqq portalocker==2.7.0\n",
    "!pip install -qq torchtext==0.14.1\n",
    "!pip install -Uq nltk==3.8.1\n",
    "\n",
    "!python -m spacy download de\n",
    "!python -m spacy download en\n",
    "\n",
    "!pip install pdfplumber==0.9.0\n",
    "!pip install fpdf==1.7.2\n",
    "\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4eecc2-730e-45a9-88d4-f2996a3a3355",
   "metadata": {},
   "source": [
    "## Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da731fc-8632-4fa0-9dd3-590fdf944f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import multi30k, Multi30k\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f1003f-3410-4ddc-b2ad-8c1a8d39542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb57794-884e-4030-88be-31fb79951383",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "In the English-German Multi30K dataset, you first load the data and break down sentences into words or smaller pieces, called tokens. From these tokens, you create a unique list or vocabulary. Each token is then turned into a specific number using this vocabulary. Because sentences can be of different lengths,  add padding to make them all the same size in a batch. All this processed data is then organized into a PyTorch DataLoader, making it easy to use for training neural networks. A function has been provided for you to handle all these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2d2b30-0ac3-479c-b4fb-0b9dd2e9a987",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'Multi30K_de_en_dataloader.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\User\\myenv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:720\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    719\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m arg_lst[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 720\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfile_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\User\\myenv\\lib\\site-packages\\IPython\\utils\\path.py:91\u001b[0m, in \u001b[0;36mget_py_filename\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m py_name\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile `\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "\u001b[1;31mOSError\u001b[0m: File `'Multi30K_de_en_dataloader.py'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMulti30K_de_en_dataloader.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2482\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2480\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2482\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2485\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2486\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\User\\myenv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:731\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,fpath):\n\u001b[0;32m    730\u001b[0m         warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Windows, use double quotes to wrap a filename: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124mun \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmypath\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmyfile.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmeta_path:\n",
      "\u001b[1;31mException\u001b[0m: File `'Multi30K_de_en_dataloader.py'` not found."
     ]
    }
   ],
   "source": [
    "%run Multi30K_de_en_dataloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed142477-bd9a-48a1-89b6-642b3682f30e",
   "metadata": {},
   "source": [
    "You've set up data loaders for training and testing. Given the exploratory work, use a batch size of one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59207cbd-f08d-4fb6-99b8-94ebd96a6036",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_translation_dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataloader, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_translation_dataloaders\u001b[49m(batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_translation_dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader, _ = get_translation_dataloaders(batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc1410-c824-4d78-810b-ac7befac5d38",
   "metadata": {},
   "source": [
    "Initialize an iterator for the validation data loader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d732ad3-0f3f-4091-aefd-3eed758cdcdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_itr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m)\n\u001b[0;32m      2\u001b[0m data_itr\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "data_itr=iter(train_dataloader)\n",
    "data_itr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb794ec0-7ffb-49e7-b71c-fd7ad85d3322",
   "metadata": {},
   "source": [
    "To obtain diverse examples, you can cycle through multiple samples since the dataset is sorted by length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a270bde-eb55-43e3-9fe2-a0cd99533af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_itr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     german, english\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mdata_itr\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_itr' is not defined"
     ]
    }
   ],
   "source": [
    "for n in range(1000):\n",
    "    german, english= next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82eadc-d9ba-4999-883a-02f8adf3747f",
   "metadata": {},
   "source": [
    "The dataset is structured as sequence-batch-feature, rather than the typical batch-feature-sequence. For compatibility with your utility functions, you can transpose the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e64c9d-67cc-47dd-a869-5282b782a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "german=german.T\n",
    "english=english.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7641955-073e-4f1e-b487-dcfb0a842fc5",
   "metadata": {},
   "source": [
    "You can print out the text by converting the indexes to words using ```index_to_german``` and ```index_to_english```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b158cd-1d14-4651-9b3f-50e2d9d8f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "    print(\"sample {}\".format(n))\n",
    "    print(\"german input\")\n",
    "    print(index_to_german(german))\n",
    "    print(\"english target\")\n",
    "    print(index_to_eng(english))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c51fb0-c0dc-41dc-8d03-dc1e2ffb456e",
   "metadata": {},
   "source": [
    "Let's define your device (CPU or GPU) for training. You'll check if a GPU is available and use it; otherwise, you'll use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb7562-5058-4dd7-9eba-f9db14f91e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4d87b-840c-4e1d-b553-cd216d81fbab",
   "metadata": {},
   "source": [
    "Now that you've covered data preparation, let's move on to understanding the key components of the transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502b460-0a2d-48e0-92db-37601c36dced",
   "metadata": {},
   "source": [
    "## Important concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b60ebd-1ce9-46d7-bc26-3b37d5af8b04",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "During training, the entire sequence is visible to the model and used as input to learn patterns. In contrast, for prediction, the future sequence is not available. To do this, employ masking to simulate this lack of future data, ensuring the model learns to predict without seeing the actual next tokens. It is crucial for ensuring certain positions are not attended to. The function ```generate_square_subsequent_mask``` produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a2c8f-9330-4e18-9aaa-196fbbe0f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709fd42-6e6d-4112-b840-a46fe1701c25",
   "metadata": {},
   "source": [
    "The ```create_mask``` function, on the other hand, generates both source and target masks, as well as padding masks based on the provided source and target sequences. The padding masks ensure that the model doesn't attend to pad tokens, providing a streamlined attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029b7f5-7bc9-474c-bab0-d992a11efd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d64b5-0d11-431b-a56a-6d94ce8bb738",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "The transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the tokens embeddings. These encodings have a fixed pattern based on their position in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd3cdc-4120-4478-acde-2802b02a7be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add positional information to the input tokens\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929979c6-2f02-4aa8-93d9-3f2f473bd800",
   "metadata": {},
   "source": [
    "### Token embedding\n",
    "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
    "\n",
    "The `TokenEmbedding` class below converts numerical tokens into embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbeb53-4580-42a6-af70-132d7bbd37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bcc32-286b-4bef-bc8f-85d3ea0cfff0",
   "metadata": {},
   "source": [
    "## Transformer architecture for language translation\n",
    "Language translation using a transformer model is a sophisticated process that relies on an encoder-decoder architecture. In this explanation, you will break down the essential components and training procedures for a clear understanding.\n",
    "\n",
    "### Tokenization and positional encoding\n",
    "To begin, source language text (the input sequence) is tokenized, which means it's divided into individual words or subwords. These tokens are then converted into numerical representations. To preserve word order information, positional encodings are added to these numerical tokens.\n",
    "\n",
    "### Encoder processing\n",
    "The next step involves passing these numerical tokens through the encoder. The encoder is composed of multiple layers, each containing self-attention mechanisms and feed-forward neural networks. This architecture allows the transformer model to process the entire input sequence at once, in contrast to traditional RNN-based models like LSTMs or GRUs, which process input sequentially.\n",
    "\n",
    "### Decoding with teacher forcing\n",
    "During training, the target language text (the correct output sequence) is also tokenized and converted into numerical tokens. \"Teacher forcing\" is a training technique where the decoder is provided with the target tokens as input. The decoder uses both the encoder's output and the previously generated tokens (starting with a special start-of-sequence token) to predict the next token in the sequence.\n",
    "\n",
    "### Output generation and loss calculation\n",
    "The decoder generates the translated sequence token by token. At each step, the decoder predicts the next token in the target sequence. The predicted sequence from the decoder is then compared to the actual target sequence using a loss function, typically cross-entropy loss for translation tasks. This loss function quantifies how well the model's predictions match the true target sequence.\n",
    "\n",
    "## Seq2SeqTransformer\n",
    "Now, let's delve into the Seq2SeqTransformer class, which represents the core of the transformer model for language translation.\n",
    "\n",
    "To train this model effectively, the following aspects will be covered:\n",
    "\n",
    "- **Data loading:** Loading and preparing the training data, which includes source language text and corresponding target language text.\n",
    "\n",
    "- **Model initialization:** Initializing the transformer model, including setting up the encoder, decoder, positional encodings, and other necessary components.\n",
    "\n",
    "- **Optimizer setup:** Choosing an appropriate optimizer, such as Adam, and defining learning rate schedules to update model parameters during training.\n",
    "\n",
    "- **Training loop:** Iterating through the training data for multiple epochs, using teacher forcing to guide the model's learning process.\n",
    "\n",
    "- **Loss monitoring:** Recording and potentially plotting training losses for each epoch. These losses indicate how well the model is learning to perform language translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341d74a-e36e-4ca0-8287-9a028eec1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        outs =outs.to(DEVICE)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278394d6-9e49-4c70-b8b8-3ce5b4a45643",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "The diagram below illustrates the sequence prediction or inference process. You can begin by feeding the indices of your desired translation sequence into the encoder, represented by the lower-left orange section. The resulting embeddings from the encoder are then channeled into the decoder, highlighted in green. Alongside, a start token is introduced at the beginning of the decoder input, as depicted at the base of the green segment.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/predict_transformers.png\" alt=\"transformer\">\n",
    "The decoder's output is then mapped onto a vocabulary-sized vector using a linear layer. Following this, a softmax function converts these vector scores into probabilities. The highest probability, as determined by the argmax function, provides the index of your predicted word within the translated sequence. This predicted index is fed back into the decoder in conjunction with the initial sequence, setting the stage to determine the subsequent word in the translation. This autoregressive process is demonstrated by the arrow pointing to form the top of the decoder, in green, to the bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa40cb4-c0fa-4928-be36-9743d92f5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cb335-e9c2-4f21-8ac2-88a0ecb31617",
   "metadata": {},
   "source": [
    "Let's will start off with a trained model.For this, load the weights of the transformer model from the file 'transformer.pt'.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ee0a7-0fbb-4000-bfb0-22df6caf1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16617746-32bb-4b97-95cf-9d19f55e1039",
   "metadata": {},
   "source": [
    "Since your dataset is organized by sequence length, let's iterate through it to obtain a longer sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa8db3-0a69-4e7e-9d72-6bbb05b6aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(100):\n",
    "    src ,tgt= next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cfb28-0ad0-4894-bd03-e8bc51cfeb88",
   "metadata": {},
   "source": [
    "Display the source sequence in German that you aim to translate, alongside the target sequence in English that you want your model to produce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8464afe-defa-4082-93a8-de9082937c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"engish target\",index_to_eng(tgt))\n",
    "print(\"german input\",index_to_german(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858e0f2-2e87-47f9-b957-82e9b1439e77",
   "metadata": {},
   "source": [
    "You will find the number of tokens in the German sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af827e7-2ad1-4262-b62e-cf4737c9fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = src.shape[0]\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6ae10-fc0d-4249-bd59-7f8863ca17de",
   "metadata": {},
   "source": [
    "You can construct a mask to delineate which inputs are factored into the attention computation. Given that this pertains to a translation task, all tokens in the source sequence are accessible, thus setting the mask values to `false`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ab8e3-5382-4af5-a8a7-3ecc96ddd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )\n",
    "src_mask[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a255211-11f4-4c2e-9f21-1d1d3ccf6f20",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Extract the first sample from the batch of sequences slated for translation. While currently redundant, this procedure will become relevant later as you handle larger batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a4c70-f023-4973-a4d9-f8f18d720633",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_=src[:,0].unsqueeze(1)\n",
    "print(src_.shape)\n",
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e417af0-ac7c-4286-b450-ba7376c1cee7",
   "metadata": {},
   "source": [
    "Feed the tokens of the sequences designated for translation into the transformer, accompanied by the mask. The resultant values, stored in the 'memory', are the embeddings derived from the output of the transformer encoder as shown in the following image: \n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/Transformersencoder.png\" alt=\"trasfoemr\">\n",
    "\n",
    "The memory, which is the encoder's output, encapsulates the original sequence to be translated and serves as the input for the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a14394-ebe7-4fe7-b9b7-e17186a34bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = transformer.encode(src_, src_mask)\n",
    "memory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147ffeb-2b37-436d-b4d0-bb4d8fc46c87",
   "metadata": {},
   "source": [
    " To indicate the beginning of an output sequence generation, initiate it with the start symbol:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b1328-3e26-48d9-866d-0be0ca6dd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f3b95-521a-443a-bf4a-33f4ae11350b",
   "metadata": {},
   "source": [
    "Due to some naming conventions, the term \"target\" is used to denote the prediction. In this context, the \"target\" refers to the words following the current prediction. These can be combined with the source to make further predictions. Consequently, you construct a target mask set to 'false' indicating that no values should be ignored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad93451-6d44-47da-94bf-fd74d17a793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086756c-2ad1-48c5-89d6-7cdab5896a05",
   "metadata": {},
   "source": [
    "You feed the encoder's output (referred to as 'memory') and the previous prediction from the transformer, which at this point is solely the start token, into the decoder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f49364-cfc1-43ef-936e-c16d41028c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer.decode(ys, memory, tgt_mask)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd894add-b0a4-43a8-9e8f-8562a6eabddf",
   "metadata": {},
   "source": [
    "The decoder's output is an enhanced word embedding representing the anticipated translation. At this point, the batch dimension is omitted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cecb6e-7662-4ac3-9e3a-1215c12dd6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.transpose(0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a204f3a-2b2f-49ff-84e9-a1d2e367e4c9",
   "metadata": {},
   "source": [
    "Once the decoder produces its output, it's passed through output layer logit value  over the vocabulary of 10837 words. Later on you will only need the last token so you can input```out[:, -1]```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08972e36-c02a-4c5e-87a7-f275d54b9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = transformer.generator(out[:, -1])\n",
    "logit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c86763-1c64-42e8-a25f-aef411475d27",
   "metadata": {},
   "source": [
    "The process is succinctly illustrated in the image below:\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/decoder_start.png\" alt=\"trasfoemr\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833d2ca-c8ff-4bf7-ad18-020507d6caf0",
   "metadata": {},
   "source": [
    "\n",
    "The predicted word is determined by identifying the highest logit value, which signifies the model's most probable translation for the input at a specific position; this position corresponds to the index of the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f54ae-5d52-4aeb-8372-a3417ac4a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  _, next_word_index = torch.max(logit, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a75d66-72b3-4dee-8029-8515f5b3f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"engish output:\",index_to_eng(next_word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c838867-68f9-4f3f-a957-8a5607f6bf9c",
   "metadata": {},
   "source": [
    "You only need the integer for the index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443a8ec-3182-44ad-8461-03da2601582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_index=next_word_index.item()\n",
    "next_word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb2249-2e48-4a83-94c8-01f21c4bfed8",
   "metadata": {},
   "source": [
    "Now, append the newly predicted word to the prior predictions, allowing the model to consider the entire sequence of generated words when making its next prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76904b-845d-4058-8ef1-fd35a85cbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49aa360-0823-4e2b-88b5-ccbba1706a1f",
   "metadata": {},
   "source": [
    "To predict the subsequent word in the translation, update target mask and use the transformer decoder to derive the word probabilities. The word with the maximum probability is then selected as the prediction. Note that the encoder output contains all the information you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2f4cc-35bf-4d2b-857f-cbfa783ac082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the target mask for the current sequence length.\n",
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288f720-e66c-4e52-a89b-af55084569b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the current sequence using the transformer and retrieve the output.\n",
    "out = transformer.decode(ys, memory, tgt_mask)\n",
    "out = out.transpose(0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88a29e-7b8d-407a-b560-bfa6a82c745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:, -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6630b2-8922-4486-aa21-d5cf9e5bd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word probabilities for the last predicted word.\n",
    "prob = transformer.generator(out[:, -1])\n",
    "# Find the word index with the highest probability.\n",
    "_, next_word_index = torch.max(prob, dim=1)\n",
    "# Print the predicted English word.\n",
    "print(\"English output:\", index_to_eng(next_word_index))\n",
    "# Convert the tensor value to a Python scalar.\n",
    "next_word_index = next_word_index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1b364-a53e-4673-81cb-7b710c901981",
   "metadata": {},
   "source": [
    "Now, update the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c8ca1-62f3-467c-9cdd-fefdcefb65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)\n",
    "print(\"engish output\",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682f694-5f1c-409b-8bfc-eec981ac31db",
   "metadata": {},
   "source": [
    "The process can be summarized as follows: \n",
    "Starting with the initial output of the encoder and the <BOS> token, the decoder's output is looped back into the decoder until the translated sequence is fully decoded. This cycle continues until the length of the new translated sequence matches that of the original sequence. As shown in the following image, the function ```greedy_decode``` is also included.\n",
    "    \n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/decoder.png\" alt=\"trasfoemr\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762a49e-0a55-41dc-8202-9695ec166f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2742b78-5db0-4819-bcd7-8938ccac7a07",
   "metadata": {},
   "source": [
    "Retrieve the indices for the German language and generate the corresponding mask:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cbe73-ef61-4e02-a5db-0d1fd2f45bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src\n",
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c53a7f-cfeb-46ae-9575-b1a31ac63c10",
   "metadata": {},
   "source": [
    "Set a reasonable value for the max length of target sequence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80019b7b-a456-42b7-beff-482e18c79686",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=src.shape[0]+5\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4c6d8-2f1f-493a-be1b-345d14674e74",
   "metadata": {},
   "source": [
    "Apply the function ```greedy_decode``` to data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eef9df-d228-4148-88d2-b3d72e5b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=greedy_decode(transformer, src, src_mask, max_len, start_symbol=BOS_IDX)\n",
    "print(\"engish \",index_to_eng(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adf6f0-ce59-4920-a2fd-bd8d60e3b8a9",
   "metadata": {},
   "source": [
    "Notice that it works, but it's not exactly the same. However, it's still pretty good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28dc11-bbf2-44cc-8eac-65fdf6bb8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"engish \",index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cba568-c89f-463a-a125-ea4ca93841cd",
   "metadata": {},
   "source": [
    "### Decoding the differences: Training vs. inference in neural machine translation\n",
    "\n",
    "During the inference phase, when the model is deployed for actual translation tasks, the decoder generates the sequence without access to the expected target sequence. Instead, it bases its predictions on the encoder's output and the tokens it has produced in sequence so far. The process is autoregressive, with the decoder continually predicting the next token until it outputs an end-of-sequence token, indicating the translation is complete.\n",
    "\n",
    "The key difference between the training and inference stages lies in the inputs to the decoder. During training, the decoder benefits from exposure to the ground truth--receiving the exact target sequence tokens incrementally through a technique known as \"teacher forcing.\" This approach is in stark contrast to some other neural network architectures that rely on the network's previous predictions as inputs during training. Once training concludes, the datasets used resemble those employed in more conventional neural network models, providing a familiar foundation for comparison and evaluation.\n",
    "\n",
    "First, import `CrossEntropyLoss` loss and create a Cross Entropy Loss object The loss will  not be calculated when the token with index `PAD_IDX` an input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16850b2f-6d6b-43b1-8591-545d17ded070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a06778-b61f-4c3a-b4e8-cac5a095a907",
   "metadata": {},
   "source": [
    "Drop the last sample of the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9181142-e285-472d-a5a2-eef7bff8805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_input = tgt[:-1, :]\n",
    "print(index_to_eng(tgt_input))\n",
    "print(index_to_eng(tgt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fc76d-ce38-4653-8074-420589ceb9fd",
   "metadata": {},
   "source": [
    "Create the required masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb99aa2-dded-4a27-ab33-51c956a7c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "print(f\"Shape of src_mask: {src_mask.shape}\")\n",
    "print(f\"Shape of tgt_mask: {tgt_mask.shape}\")\n",
    "print(f\"Shape of src_padding_mask: {src_padding_mask.shape}\")\n",
    "print(f\"Shape of tgt_padding_mask: {tgt_padding_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b225ce4-81e9-45fe-b27c-efe9d6831517",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3848ff-3ed3-442b-944f-f137bb7cdd38",
   "metadata": {},
   "source": [
    "In the target mask, each subsequent column incrementally reveals more tokens by introducing negative infinity values, thereby unblocking them. You can display the target mask to visualize the progression or specifically identify which tokens are being masked at each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf7346-ba05-436d-aec2-b46a24f78a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tgt_mask)\n",
    "[index_to_eng( tgt_input[t==0])  for t in tgt_mask] #index_to_eng(tgt_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a2ea2-a7af-421a-bb9a-6b91727e162e",
   "metadata": {},
   "source": [
    "When you call `model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)`,  the forward method of the `Seq2SeqTransformer` class. This process generates logits for the target sequence, which can then be translated into actual tokens by taking the highest probability prediction at each step in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24b591-45c3-4329-bea7-b65f20d144db",
   "metadata": {},
   "source": [
    "## Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefaedc-8d5e-4c36-a103-df7679b277d7",
   "metadata": {},
   "source": [
    "Let's delve into how you can calculate the loss, you have your  ```src``` and  ```tgt_input```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50dd55-e45e-4704-b3f2-a7f3234da863",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "print(\"output shape\",logits.shape)\n",
    "print(\"target shape\",tgt_input.shape)\n",
    "print(\"source shape \",src.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2f27a-9169-469b-9bf2-f0a2c29defa7",
   "metadata": {},
   "source": [
    "\n",
    "During the training phase, an intriguing and sophisticated aspect of the process is the dual functionality of the target. It simultaneously acts as the input for the transformer's decoder and as the standard against which the prediction's accuracy is measured.  For clarity in the discussions, you'll refer to the target used as the input for the decoder as the \"Input to the Decoder.\" On the other hand, the \"Ground Truth for Prediction,\" which is a the target sequence shifted to the right, as it's an auto regressive model. This will be simply known as the \"Target out\" moving forward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c2fa3-d122-407d-a448-06174c0e1037",
   "metadata": {},
   "source": [
    "Ground Truth for Prediction is simply shifted right and is called ```tgt_out``` , you can print out tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22951870-51d6-4151-a814-c7b4b60a188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_out = tgt[1:, :]\n",
    "print(tgt_out.shape)\n",
    "[index_to_eng(t)  for t in tgt_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5b4bc-c4ae-4d6f-af55-381092bd4f50",
   "metadata": {},
   "source": [
    "\n",
    "The token indices represent the classes you aim to predict. By flattening the tensor, each index becomes a distinct sample, serving as the target for the cross-entropy loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe145d2-9e1f-4d60-bf53-4d7cdb3c1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_out_flattened = tgt_out.reshape(-1)\n",
    "print(tgt_out_flattened.shape)\n",
    "tgt_out_flattened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ffd44-e5ed-4d7f-94b9-9d996030fb5d",
   "metadata": {},
   "source": [
    "In this autoregressive model,  showcase the input target tokens after the application of the mask. Beside them, you can display the target output, illustrating how the model adeptly predicts past values based on present ones. This clear visualization highlights the model's capability to use current information to infer what has preceded, a key feature of its autoregressive nature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c780bd4-9bba-4638-a0ad-9670ccee2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[\"input: {} target: {}\".format(index_to_eng( tgt_input[m==0]),index_to_eng( t))  for m,t in zip(tgt_mask,tgt_out)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6e3b3-5878-424d-8788-c44264b895c9",
   "metadata": {},
   "source": [
    "Now, calculate the loss as the output from the transformer's decoder is provided as input to the cross-entropy loss function along with the target sequence values. Given that the transformer's output has the dimensions sequence length, batch size, and features (vocab_size), it's necessary to reshape this output to align with the standard input format required by the cross-entropy loss function. This step ensures that the loss is calculated correctly, comparing the predicted sequence against the ground truth at each time step across the batch using the reshape method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd80e22-7314-412d-b81c-339812d92e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48c4e1-a590-48c2-a5db-8c682b5e6b3c",
   "metadata": {},
   "source": [
    "### Under the hood of loss calculation (Optional)\n",
    "That's it for loss calculation, but if you are curious how the loss is calculated here is what happens under the hood of calculating the Cross Entropy loss. First, check the shape of tensors before and after the reshaping:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62c758-4b1e-4cd4-b2af-7a5d7ea5e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits.reshape(-1, logits.shape[-1]) reshapes the logits tensor to a 2D tensor with a shape of [sequence_length * batch_size, vocab_size]. This reshaping is done to align both the predicted logits and target outputs for the loss calculation.\n",
    "print(\"logit's shape is:\",logits.shape)\n",
    "logits_flattened = logits.reshape(-1, logits.shape[-1])\n",
    "print(\"logit_flat's shape is:\",logits_flattened.shape)\n",
    "\n",
    "\n",
    "# tgt_out.reshape(-1) reshapes the tgt_out tensor to a 1D tensor by flattening it along the sequence and batch dimensions. This is done to align it with the reshaped logits.\n",
    "print(\"tgt_out's shape is:\",tgt_out.shape)\n",
    "tgt_out_flattened = tgt_out.reshape(-1)\n",
    "print(\"tgt_out_flat's shape is:\",tgt_out_flattened.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bbf83-e96a-49c3-b99a-d3b32121c5de",
   "metadata": {},
   "source": [
    "Inside the loss function, logits will transform into probabilities between [0,1] that sum up to 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a670ab-a60e-473f-8b7d-45f7916f6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Cross-Entropy Loss Function\n",
    "probs = torch.nn.functional.softmax(logits_flattened, dim=1)\n",
    "probs[1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0f8af-ae32-41f1-9b18-5842ca231377",
   "metadata": {},
   "source": [
    "let's check the probabilities for some random tokens:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1dc6d36-8b4a-4c6e-b9cd-705b1f8eedff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dd8c8-606c-41b7-a117-2182e2583283",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    # using argmax, you can retrieve the index of the token that is predicted with the highest probaility\n",
    "    print(\"Predicted token id:\",probs[i].argmax().item(), \"predicted probaility:\",probs[i].max().item())\n",
    "    # you can also check the actual token from the tgt_out_flat\n",
    "    print(\"Actual token id:\",tgt_out_flattened[i].item(), \"predicted probaility:\", probs[i,tgt_out_flattened[i]].item(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd0cce-5912-4c43-a1df-9b96ef4ef0fd",
   "metadata": {},
   "source": [
    "It can be seen that for many tokens the model is doing a good job in predicting the token, while for some of the tokens(for example the third block in the output above) the model is not assigning a high probability to the actual token to be predicted. The difference between the predicted probability for such tokens is the reason why the loss would not sum up to 0.\n",
    "\n",
    "Now, you can proceed with calculating the difference between the actual token's probability (1) and the predicted probabilities for each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce18e6a-8bf8-4954-87a1-4e0be1465c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_likelihood = torch.nn.functional.nll_loss(probs, tgt_out_flattened)\n",
    "# Step 3: Obtaining the Loss Value\n",
    "loss = neg_log_likelihood\n",
    "\n",
    "# Print the total loss value\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eda021-1ef4-400c-bfd0-043219de24a3",
   "metadata": {},
   "source": [
    "## Evaluate \n",
    "By following the aforementioned procedures, you can develop a function that is capable of making predictions and subsequently computing the corresponding loss on the validation data, you will use this function later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9973a6-2860-460b-909d-abcc1d852f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d0589-3865-41f0-8e6a-aa4ea7517014",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Incorporating the previously outlined steps, proceed to train the model. Apart from these specific procedures, the overall training process conforms to the conventional methods employed in neural network training. Now, write a function to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de1480-8f43-4752-b795-9b52d183f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_dataloader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    # Wrap train_dataloader with tqdm for progress logging\n",
    "    train_iterator = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for src, tgt in train_iterator:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "        tgt_mask = tgt_mask.to(DEVICE)\n",
    "        src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "        tgt_padding_mask = tgt_padding_mask.to(DEVICE)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        logits = logits.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84826a74-4cf2-483b-9cc5-3733124526de",
   "metadata": {},
   "source": [
    "The configuration for the translation model includes a source and target vocabulary size determined by the dataset languages, an embedding size of 512, 8 attention heads, a hidden dimension for the feed-forward network of 512, and a batch size of 128. The model is structured with three layers each in both the encoder and the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afeb94d-05e1-4002-a7bd-b9d2dde1df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a9cd3-5683-4830-aeb8-03e7caa08bdb",
   "metadata": {},
   "source": [
    "Create a train loader with a batch size of 128.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae8c16-a93a-4051-9d89-c9dfe40178dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64b59c-0fbd-486a-bbba-a413f8ea1ec2",
   "metadata": {},
   "source": [
    "Create a transformer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ec9e9-fcf1-4b81-af5b-36a8201184fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ec650-98d2-4521-943b-df2b8411e39e",
   "metadata": {},
   "source": [
    "Initialize the weights of the transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e053eda-a687-499c-8cf7-38a39005f384",
   "metadata": {},
   "source": [
    "Insulate the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1aacf-dc9a-4a6e-9856-53f8904a01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438a024-c5fe-458f-b32a-e5d52392ebb7",
   "metadata": {},
   "source": [
    "Initialize the train loss and validation loss list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c14a4-dd40-4519-8632-8041766c8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoss=[]\n",
    "ValLoss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2b743-cc7c-4327-b9b3-50293c0ee48a",
   "metadata": {},
   "source": [
    "Train the model for 10 epochs using the above functions.\n",
    "\n",
    "Please be aware that training the model using CPUs can be a time-consuming process. If you don't have access to GPUs, you can jump to  \"loading the saved model\" and proceed with loading the pretrained model using the provided code. You have been provided with the saved model that has been trained for 40 epochs. \n",
    "> The Skills Network Lab environment uses CPU. The training time for each epoch can anywhere between 40 minutes to an hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116c441-c3eb-4061-b6ac-16244bf0c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_dataloader)\n",
    "    TrainLoss.append(train_loss)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    ValLoss.append(val_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de97bda-6dc5-4451-8499-72a622c59928",
   "metadata": {},
   "source": [
    "Plot the loss for the  training and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff61b8-862c-4833-bcae-b12da486faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(TrainLoss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, TrainLoss, 'r', label='Training loss')\n",
    "plt.plot(epochs,ValLoss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2640fa4-7900-4740-89d1-fc690c93b349",
   "metadata": {},
   "source": [
    "## Loading the saved model\n",
    "If you want to skip training and load the pretrained model that is provided, go ahead and uncomment the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6d9ba-358b-4faa-8a55-eafdecc2c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'\n",
    "transformer.load_state_dict(torch.load('transformer_de_to_en_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c77977-45e8-4424-845d-4406355895f9",
   "metadata": {},
   "source": [
    "## Translation and evaluation \n",
    "Using the greedy_decode function that you defined earlier, you can create a translator function that generates English translation of an input German text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dce4af-90df-493c-a986-b34f8ae518c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a49e68-706d-4cb6-8107-92dd1d71aa3c",
   "metadata": {},
   "source": [
    "Now, let's look into some sample translations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff29dc-ac12-4b4b-979d-1429b7396886",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "    print(\"German Sentence:\",index_to_german(german).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"English Translation:\",index_to_eng(english).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    print(\"Model Translation:\",translate(transformer,index_to_german(german)))\n",
    "    print(\"_________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd9c61-6356-4a5a-8b3e-946cfcd9e926",
   "metadata": {},
   "source": [
    "### Evaluation with BLEU score\n",
    "To evaluate the generated translations, a function calculate_bleu_score is introduced. It computes the BLEU score, a common metric for machine translation quality, by comparing the generated translation to reference translations. The BLEU score provides a quantitative measure of translation accuracy.\n",
    "\n",
    "The code also includes an example of calculating the BLEU score for a generated translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a78f75-31d2-488b-88a1-9eb92fd6d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "    # convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "    references = [reference.split() for reference in reference_translations]\n",
    "    hypothesis = generated_translation.split()\n",
    "\n",
    "    # calculate the BLEU score\n",
    "    bleu_score = sentence_bleu(references, hypothesis)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b004d2-eeae-4510-ac84-2b4ddfaa18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_translation = translate(transformer,\"Ein brauner Hund spielt im Schnee .\")\n",
    "\n",
    "reference_translations = [\n",
    "    \"A brown dog is playing in the snow .\",\n",
    "    \"A brown dog plays in the snow .\",\n",
    "    \"A brown dog is frolicking in the snow .\",\n",
    "    \"In the snow, a brown dog is playing .\"\n",
    "\n",
    "]\n",
    "\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
    "print(\"BLEU Score:\", bleu_score, \"for\",generated_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b107c9e-40fc-476b-bd7e-8165aff907ea",
   "metadata": {},
   "source": [
    "## Exercise: Translating a document\n",
    "In this exercise, you will implement a feature that translates a PDF in German to English. To achieve this, you will leverage the same sequence-to-sequence transformer model discussed previously and make necessary modifications.\n",
    "\n",
    "1. **Define the translation function**:\n",
    "   Create a function named `translate_pdf` that takes the following parameters:\n",
    "   - `input_file`: The path to the input PDF file to be translated.\n",
    "   - `translator_model`: A model or function that will handle the translation of text.\n",
    "   - `output_file`: The path where the translated PDF will be saved.\n",
    "\n",
    "2. **Read and translate the PDF**:\n",
    "   Use `pdfplumber` to open and read the text from each page of the input PDF. Translate the extracted text using the `translator_model`.\n",
    "\n",
    "3. **Format and write the translated text to a new PDF**:\n",
    "   - Use `textwrap` to wrap the translated text so that it fits within the A4 page width.\n",
    "   - Create a new PDF with `FPDF` and add the wrapped translated text to it.\n",
    "   - Save the new PDF with the translated text to `output_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948ecad-a3f7-4486-a521-94b5b86c894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import textwrap\n",
    "from fpdf import FPDF\n",
    "\n",
    "def translate_pdf(input_file, translator_model,output_file):\n",
    "    #Complete the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900d475-81b9-4248-b54d-b59242278ad1",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "def translate_pdf(input_file, translator_model,output_file):\n",
    "    translated_text = \"\"\n",
    "\n",
    "    # Read the input PDF file\n",
    "    with pdfplumber.open(input_file) as pdf:\n",
    "\n",
    "\n",
    "        # Extract text from each page of the PDF\n",
    "        for page in pdf.pages:\n",
    "            text_content = page.extract_text()\n",
    "            num_pages = len(pdf.pages)\n",
    "            a4_width_mm = 210\n",
    "            pt_to_mm = 0.35\n",
    "            fontsize_pt = 10\n",
    "            fontsize_mm = fontsize_pt * pt_to_mm\n",
    "            margin_bottom_mm = 10\n",
    "            character_width_mm = 7 * pt_to_mm\n",
    "            width_text = a4_width_mm / character_width_mm\n",
    "\n",
    "            pdf = FPDF(orientation='P', unit='mm', format='A4')\n",
    "            pdf.set_auto_page_break(True, margin=margin_bottom_mm)\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(family='Courier', size=fontsize_pt)\n",
    "            # Split the text into sentences\n",
    "            sentences = text_content.split(\".\")\n",
    "\n",
    "            # Translate each sentence using the custom translator model\n",
    "            for sentence in sentences:\n",
    "                translated_sentence = translate(translator_model,sentence)\n",
    "                lines = textwrap.wrap(translated_sentence, width_text)\n",
    "\n",
    "                if len(lines) == 0:\n",
    "                    pdf.ln()\n",
    "\n",
    "                for wrap in lines:\n",
    "                    pdf.cell(0, fontsize_mm, wrap, ln=1)\n",
    "\n",
    "            pdf.output(output_file, 'F')\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96b1c3-5604-43e0-a174-51b254766038",
   "metadata": {},
   "source": [
    "Here is a German document for you to convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0f013-2e42-4f28-85a4-6e15c4d7c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c019c-85d9-4760-b7f1-81476731061d",
   "metadata": {},
   "source": [
    "Now call the translate_pdf for the German file as an input to the function and check the output file for the translated file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcde92-dab8-492a-a523-4bd006cc7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = #Input file path\n",
    "output_file = 'output_en.pdf'\n",
    "# Call the translate_pdf() defined earlier for the input file\n",
    "print(\"Translated PDF file is saved as:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d612eed-2a4f-45fe-854b-6ca6e83cdb92",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "input_file_path = \"input_de.pdf\"\n",
    "output_file = 'output_en.pdf'\n",
    "translate_pdf(input_file_path, transformer,output_file)\n",
    "print(\"Translated PDF file is saved as:\", output_file)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839471f-0c47-4add-a25c-8387f542d1aa",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a126dc8-5dd4-499f-840a-cffead20507f",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a Ph.D. candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b600b-3b92-444c-8182-1804f6959de6",
   "metadata": {},
   "source": [
    "## References\n",
    "[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper.\n",
    "\n",
    "[Language Translation with nn.Transformer and torchtext](https://pytorch.org/tutorials/beginner/translation_transformer.html) PyTorch tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888c334-7b6e-43f6-a8c9-4636d98c659b",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "prev_pub_hash": "f583ab330d392f3fbc803e1d84830f575a94e0d7cc0f8b3af49ded45fd51cc14"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
